#!/usr/bin/env python
# coding: utf-8

from dataclasses import dataclass
from pathlib import Path
from astropy.io import fits
from astropy.time import Time
import astropy.units as u
import pandas as pd
from tqdm import tqdm


@dataclass
class VastNeighbour:
    field: str
    sbid: int
    filename: str
    date_start_isot: str
    date_end_isot: str
    combined_field: str
    combined_release_epoch: str
    combined_main_field: bool


def combined_date_obs(df) -> str:
    main_field_obs = df.query("combined_main_field").reset_index().set_index("sbid")
    idx = main_field_obs.date_start_isot.idxmin()
    return main_field_obs.loc[idx][["date_start_isot", "date_end_isot"]]


VAST_REPO = Path("/data/VAST/askap-surveys-database/vast/db")
RACS_REPO = Path("/data/VAST/askap-surveys-database/racs/db")

# read the VAST metadata
vast_df = pd.DataFrame()
for field_data_path in VAST_REPO.glob("epoch_*/field_data.csv"):
    vast_df = vast_df.append(pd.read_csv(field_data_path))
vast_df["DATE-BEG"] = pd.to_datetime(Time(vast_df["SCAN_START"].values * u.s, format="mjd").isot)
vast_df["DATE-END"] = vast_df["DATE-BEG"] + pd.to_timedelta(vast_df["SCAN_LEN"], unit="sec")

# read the RACS metadata
racs_df = pd.DataFrame()
for field_data_path in RACS_REPO.glob("epoch_[01]/field_data.csv"):
    racs_df = racs_df.append(pd.read_csv(field_data_path))
racs_df["DATE-BEG"] = pd.to_datetime(Time(racs_df["SCAN_START"].values * u.s, format="mjd").isot)
racs_df["DATE-END"] = racs_df["DATE-BEG"] + pd.to_timedelta(racs_df["SCAN_LEN"], unit="sec")
racs_df["FIELD_NAME"] = racs_df["FIELD_NAME"].str.replace("RACS", "VAST")

# add racs to vast
vast_df = pd.concat((vast_df, racs_df))

# remove duplicates
vast_df = vast_df.drop_duplicates(subset=["FIELD_NAME", "SBID"], keep=False)
vast_df = vast_df[["FIELD_NAME", "SBID", "DATE-BEG", "DATE-END"]].set_index(["FIELD_NAME", "SBID"]).sort_index()

# get the neighbours from the hard-links generated by ~/vast-post-processing/link_neighbours.py
neighbours_list = []
for field_dir_path in Path("/data/.staging/convolved/").glob("EPOCH*/VAST_*"):
    combined_field = field_dir_path.name
    combined_release_epoch = field_dir_path.parent.name

    for input_field_path in (field_dir_path / "inputs").glob("image.i.VAST_*.fits"):
        _, _, field, sbid_str, *_ = input_field_path.name.split(".")
        sbid = int(sbid_str[2:])
        # use the metadata instead of the imager header, some of the image headers appeared incorrect
        # e.g. duplicate field observation had the same DATE-OBS which is impossible!
        date_start_meta = Time(vast_df.loc[(field, sbid), "DATE-BEG"].isoformat())
        date_end_meta = Time(vast_df.loc[(field, sbid), "DATE-END"].isoformat())
        neighbours_list.append(
            VastNeighbour(
                field=field,
                sbid=sbid,
                filename=input_field_path.name,
                date_start_isot=pd.Timestamp(date_start_meta.utc.isot),
                date_end_isot=pd.Timestamp(date_end_meta.utc.isot),
                combined_field=combined_field,
                combined_release_epoch=combined_release_epoch,
                combined_main_field=(field == combined_field),
            )
        )
neighbours_df = pd.DataFrame(neighbours_list).set_index(["combined_field", "combined_release_epoch"]).sort_index()

combined_timestamps = neighbours_df.groupby(["combined_field", "combined_release_epoch"]).apply(combined_date_obs)

# update headers
epochs = "EPOCH14"  # "EPOCH*"
for image_path in tqdm(list(Path("/data/VAST/vast-data/COMBINED/STOKESI_IMAGES/").glob(f"{epochs}/VAST_*.fits"))):
    combined_field = image_path.name.split(".")[0]
    combined_release_epoch = image_path.parent.name
    with fits.open(image_path, mode="update") as hdul:
        date_start = Time(
            combined_timestamps.loc[(combined_field, combined_release_epoch), "date_start_isot"]
        )
        date_end = Time(
            combined_timestamps.loc[(combined_field, combined_release_epoch), "date_end_isot"]
        )
        hdul[0].header["DATE-OBS"] = date_start.utc.isot
        hdul[0].header["DATE-BEG"] = date_start.utc.isot
        hdul[0].header["DATE-END"] = date_end.utc.isot
        hdul[0].header["MJD-OBS"] = date_start.utc.mjd
        hdul[0].header["MJD-BEG"] = date_start.utc.mjd
        hdul[0].header["MJD-END"] = date_end.utc.mjd
        #hdul[0].header.add_history("Set dates to earliest observation of the main central field. Edges may contain data from other dates.")
        # closing the file will save the changes, i.e. when the "with" block exits

